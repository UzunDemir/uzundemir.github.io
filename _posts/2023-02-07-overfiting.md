---
layout: post
title:  "Фундаментальные концепции переобучения и недообучения в машинном обучении."
---

Этот модуль дает интуитивно понятное введение в очень фундаментальные концепции переобучения и недообучения в машинном обучении.

Модели машинного обучения никогда не могут делать идеальные прогнозы: ошибка теста никогда не равна нулю. Этот провал происходит из -за фундаментального компромисса 

между гибкостью моделирования и ограниченным размером обучающего набора данных .

![3](https://user-images.githubusercontent.com/94790150/217064448-2a1c9334-313e-4d25-89f0-2de816595645.png)
 
*"Это и ежу понятно!" Картинка сгенерирована нейросетью. Больше таких картинок на телеграм-канале @ai_drowing*

Первая презентация определит эти проблемы и охарактеризует, как и почему они возникают.

Затем мы представим методологию количественной оценки этих проблем путем сравнения ошибки train с ошибкой test для различного выбора семейства моделей и параметров 

модели. Что еще более важно, мы подчеркнем влияние размера обучающей выборки на этот компромисс .

Наконец, мы свяжем переоснащение и недообучение с понятиями статистической дисперсии и систематической ошибки.

Готовый ноутбук с датасетом - [здесь.](https://github.com/UzunDemir/SELECTING-THE-BEST-MODEL) 
